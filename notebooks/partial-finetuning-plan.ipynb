{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858d7efb",
   "metadata": {},
   "source": [
    "Model fair fine-tuning check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07d543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING RESNET18\n",
      "============================================================\n",
      "Total parameters: 11,689,512\n",
      "\n",
      "Main components:\n",
      "  conv1: Conv2d (9,408 params)\n",
      "  bn1: BatchNorm2d (128 params)\n",
      "  relu: ReLU (0 params)\n",
      "  maxpool: MaxPool2d (0 params)\n",
      "  layer1: Sequential (147,968 params)\n",
      "  layer2: Sequential (525,568 params)\n",
      "  layer3: Sequential (2,099,712 params)\n",
      "  layer4: Sequential (8,393,728 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  fc: Linear (513,000 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for ResNet18:\n",
      "   Unfreeze: layer4 (entire final stage)\n",
      "   Parameters to train: 8,906,728\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING RESNET50\n",
      "============================================================\n",
      "Total parameters: 25,557,032\n",
      "\n",
      "Main components:\n",
      "  conv1: Conv2d (9,408 params)\n",
      "  bn1: BatchNorm2d (128 params)\n",
      "  relu: ReLU (0 params)\n",
      "  maxpool: MaxPool2d (0 params)\n",
      "  layer1: Sequential (215,808 params)\n",
      "  layer2: Sequential (1,219,584 params)\n",
      "  layer3: Sequential (7,098,368 params)\n",
      "  layer4: Sequential (14,964,736 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  fc: Linear (2,049,000 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for ResNet50:\n",
      "   Unfreeze: layer4 (entire final stage)\n",
      "   Parameters to train: 17,013,736\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING EFFICIENTNET-B0\n",
      "============================================================\n",
      "Total parameters: 5,288,548\n",
      "\n",
      "Main components:\n",
      "  features: Sequential (4,007,548 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  classifier: Sequential (1,281,000 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for EfficientNet-B0:\n",
      "   Total feature blocks: 9\n",
      "   Unfreeze: features[7], features[8] + classifier\n",
      "   Parameters to train: 2,410,392\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING INCEPTIONV3\n",
      "============================================================\n",
      "Total parameters: 27,161,264\n",
      "\n",
      "Main components:\n",
      "  Conv2d_1a_3x3: BasicConv2d (928 params)\n",
      "  Conv2d_2a_3x3: BasicConv2d (9,280 params)\n",
      "  Conv2d_2b_3x3: BasicConv2d (18,560 params)\n",
      "  maxpool1: MaxPool2d (0 params)\n",
      "  Conv2d_3b_1x1: BasicConv2d (5,280 params)\n",
      "  Conv2d_4a_3x3: BasicConv2d (138,624 params)\n",
      "  maxpool2: MaxPool2d (0 params)\n",
      "  Mixed_5b: InceptionA (255,904 params)\n",
      "  Mixed_5c: InceptionA (277,472 params)\n",
      "  Mixed_5d: InceptionA (285,152 params)\n",
      "  Mixed_6a: InceptionB (1,153,280 params)\n",
      "  Mixed_6b: InceptionC (1,297,408 params)\n",
      "  Mixed_6c: InceptionC (1,691,008 params)\n",
      "  Mixed_6d: InceptionC (1,691,008 params)\n",
      "  Mixed_6e: InceptionC (2,141,952 params)\n",
      "  AuxLogits: InceptionAux (3,326,696 params)\n",
      "  Mixed_7a: InceptionD (1,698,304 params)\n",
      "  Mixed_7b: InceptionE (5,044,608 params)\n",
      "  Mixed_7c: InceptionE (6,076,800 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  dropout: Dropout (0 params)\n",
      "  fc: Linear (2,049,000 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for InceptionV3:\n",
      "   Unfreeze: ['Mixed_7a', 'Mixed_7b', 'Mixed_7c'] + fc\n",
      "   Parameters to train: 14,868,712\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING VGG16\n",
      "============================================================\n",
      "Total parameters: 138,357,544\n",
      "\n",
      "Main components:\n",
      "  features: Sequential (14,714,688 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  classifier: Sequential (123,642,856 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for VGG16:\n",
      "   Unfreeze: last 2 conv layers + classifier\n",
      "   Parameters to train: 128,362,472\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING VGG19\n",
      "============================================================\n",
      "Total parameters: 143,667,240\n",
      "\n",
      "Main components:\n",
      "  features: Sequential (20,024,384 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  classifier: Sequential (123,642,856 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for VGG19:\n",
      "   Unfreeze: last 2 conv layers + classifier\n",
      "   Parameters to train: 128,362,472\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING MOBILENETV2\n",
      "============================================================\n",
      "Total parameters: 3,504,872\n",
      "\n",
      "Main components:\n",
      "  features: Sequential (2,223,872 params)\n",
      "  classifier: Sequential (1,281,000 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for MobileNetV2:\n",
      "   Total feature blocks: 19\n",
      "   Unfreeze: features[16:] + classifier\n",
      "   Parameters to train: 2,487,080\n",
      "\n",
      "============================================================\n",
      "ðŸ” ANALYZING MOBILENETV3-LARGE\n",
      "============================================================\n",
      "Total parameters: 5,483,032\n",
      "\n",
      "Main components:\n",
      "  features: Sequential (2,971,952 params)\n",
      "  avgpool: AdaptiveAvgPool2d (0 params)\n",
      "  classifier: Sequential (2,511,080 params)\n",
      "\n",
      "ðŸŽ¯ FINE-TUNING RECOMMENDATION for MobileNetV3-Large:\n",
      "   Total feature blocks: 17\n",
      "   Unfreeze: last 3 feature blocks + classifier\n",
      "   Parameters to train: 4,261,320\n",
      "\n",
      "âš ï¸  Xception: Not available in torchvision, needs separate handling\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FINE-TUNING PARAMETER COMPARISON\n",
      "================================================================================\n",
      "ResNet18            :  8,906,728 params ( 76.2%)\n",
      "ResNet50            : 17,013,736 params ( 66.6%)\n",
      "EfficientNet-B0     :  2,410,392 params ( 45.6%)\n",
      "InceptionV3         : 14,868,712 params ( 54.7%)\n",
      "VGG16               : 128,362,472 params ( 92.8%)\n",
      "VGG19               : 128,362,472 params ( 89.3%)\n",
      "MobileNetV2         :  2,487,080 params ( 71.0%)\n",
      "MobileNetV3-Large   :  4,261,320 params ( 77.7%)\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FINAL RECOMMENDATIONS FOR FAIR COMPARISON\n",
      "================================================================================\n",
      "To ensure fair comparison, aim for ~5-15% of total parameters to be trainable.\n",
      "Adjust the number of unfrozen layers to achieve similar parameter counts.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_model_architecture(model, model_name):\n",
    "    \"\"\"Analyze model architecture to determine last stage/block for consistent fine-tuning\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ” ANALYZING {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Count total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Show main structure\n",
    "    print(\"\\nMain components:\")\n",
    "    for name, module in model.named_children():\n",
    "        param_count = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"  {name}: {type(module).__name__} ({param_count:,} params)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_last_stages():\n",
    "    \"\"\"Check the last stages of all models for equivalent fine-tuning\"\"\"\n",
    "    \n",
    "    models_to_check = {\n",
    "        'ResNet18': models.resnet18(weights=ResNet18_Weights.DEFAULT),\n",
    "        'ResNet50': models.resnet50(weights=ResNet50_Weights.DEFAULT),\n",
    "        'EfficientNet-B0': models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT),\n",
    "        'InceptionV3': models.inception_v3(weights=Inception_V3_Weights.DEFAULT),\n",
    "        'VGG16': models.vgg16(weights=VGG16_Weights.DEFAULT),\n",
    "        'VGG19': models.vgg19(weights=VGG19_Weights.DEFAULT),\n",
    "        'MobileNetV2': models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT),\n",
    "        'MobileNetV3-Large': models.mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT),\n",
    "        'Xception': None,  # Not in torchvision, will handle separately\n",
    "    }\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    for name, model in models_to_check.items():\n",
    "        if model is None:\n",
    "            print(f\"\\nâš ï¸  {name}: Not available in torchvision, needs separate handling\")\n",
    "            continue\n",
    "            \n",
    "        analyze_model_architecture(model, name)\n",
    "        \n",
    "        # Specific analysis for each architecture\n",
    "        if 'resnet' in name.lower():\n",
    "            print(f\"\\nðŸŽ¯ FINE-TUNING RECOMMENDATION for {name}:\")\n",
    "            print(f\"   Unfreeze: layer4 (entire final stage)\")\n",
    "            layer4_params = sum(p.numel() for p in model.layer4.parameters())\n",
    "            fc_params = sum(p.numel() for p in model.fc.parameters())\n",
    "            print(f\"   Parameters to train: {layer4_params + fc_params:,}\")\n",
    "            recommendations[name] = {\n",
    "                'strategy': 'layer4 + fc',\n",
    "                'params': layer4_params + fc_params,\n",
    "                'code': 'for name, param in model.named_parameters():\\n    if \"layer4\" in name or \"fc\" in name:\\n        param.requires_grad = True'\n",
    "            }\n",
    "            \n",
    "        elif 'efficientnet' in name.lower():\n",
    "            print(f\"\\nðŸŽ¯ FINE-TUNING RECOMMENDATION for {name}:\")\n",
    "            print(f\"   Total feature blocks: {len(model.features)}\")\n",
    "            # For equivalent fine-tuning, unfreeze last 2-3 blocks\n",
    "            last_blocks = [7, 8]  # Last 2 blocks\n",
    "            last_block_params = sum(sum(p.numel() for p in model.features[i].parameters()) for i in last_blocks)\n",
    "            classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "            print(f\"   Unfreeze: features[7], features[8] + classifier\")\n",
    "            print(f\"   Parameters to train: {last_block_params + classifier_params:,}\")\n",
    "            recommendations[name] = {\n",
    "                'strategy': 'features[7:] + classifier',\n",
    "                'params': last_block_params + classifier_params,\n",
    "                'code': 'for i in [7, 8]:\\n    for param in model.features[i].parameters():\\n        param.requires_grad = True\\nfor param in model.classifier.parameters():\\n    param.requires_grad = True'\n",
    "            }\n",
    "            \n",
    "        elif 'mobilenet' in name.lower():\n",
    "            print(f\"\\nðŸŽ¯ FINE-TUNING RECOMMENDATION for {name}:\")\n",
    "            if hasattr(model, 'features'):\n",
    "                print(f\"   Total feature blocks: {len(model.features)}\")\n",
    "                # For MobileNetV2, unfreeze last 3-4 blocks for equivalence\n",
    "                if 'v2' in name.lower():\n",
    "                    last_blocks = list(range(16, 19))  # features[16], [17], [18]\n",
    "                    last_block_params = sum(sum(p.numel() for p in model.features[i].parameters()) for i in last_blocks)\n",
    "                    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "                    print(f\"   Unfreeze: features[16:] + classifier\")\n",
    "                    print(f\"   Parameters to train: {last_block_params + classifier_params:,}\")\n",
    "                    recommendations[name] = {\n",
    "                        'strategy': 'features[16:] + classifier',\n",
    "                        'params': last_block_params + classifier_params,\n",
    "                        'code': 'for i in range(16, len(model.features)):\\n    for param in model.features[i].parameters():\\n        param.requires_grad = True'\n",
    "                    }\n",
    "                else:  # MobileNetV3\n",
    "                    last_block_params = sum(sum(p.numel() for p in block.parameters()) for block in model.features[-3:])\n",
    "                    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "                    print(f\"   Unfreeze: last 3 feature blocks + classifier\")\n",
    "                    print(f\"   Parameters to train: {last_block_params + classifier_params:,}\")\n",
    "                    recommendations[name] = {\n",
    "                        'strategy': 'features[-3:] + classifier',\n",
    "                        'params': last_block_params + classifier_params,\n",
    "                        'code': 'for block in model.features[-3:]:\\n    for param in block.parameters():\\n        param.requires_grad = True'\n",
    "                    }\n",
    "            \n",
    "        elif 'vgg' in name.lower():\n",
    "            print(f\"\\nðŸŽ¯ FINE-TUNING RECOMMENDATION for {name}:\")\n",
    "            # VGG: unfreeze last conv block + classifier\n",
    "            # Find the last few conv layers\n",
    "            conv_layers = [i for i, layer in enumerate(model.features) if isinstance(layer, nn.Conv2d)]\n",
    "            last_conv_indices = conv_layers[-2:]  # Last 2 conv layers\n",
    "            last_conv_params = sum(sum(p.numel() for p in model.features[i].parameters()) for i in last_conv_indices)\n",
    "            classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "            print(f\"   Unfreeze: last 2 conv layers + classifier\")\n",
    "            print(f\"   Parameters to train: {last_conv_params + classifier_params:,}\")\n",
    "            recommendations[name] = {\n",
    "                'strategy': 'last 2 conv + classifier',\n",
    "                'params': last_conv_params + classifier_params,\n",
    "                'code': f'# Unfreeze conv layers {last_conv_indices} and classifier'\n",
    "            }\n",
    "            \n",
    "        elif 'inception' in name.lower():\n",
    "            print(f\"\\nðŸŽ¯ FINE-TUNING RECOMMENDATION for {name}:\")\n",
    "            # InceptionV3: unfreeze Mixed_7a, Mixed_7b, Mixed_7c (final inception blocks)\n",
    "            final_blocks = ['Mixed_7a', 'Mixed_7b', 'Mixed_7c']\n",
    "            final_block_params = 0\n",
    "            for block_name in final_blocks:\n",
    "                if hasattr(model, block_name):\n",
    "                    final_block_params += sum(p.numel() for p in getattr(model, block_name).parameters())\n",
    "            fc_params = sum(p.numel() for p in model.fc.parameters())\n",
    "            print(f\"   Unfreeze: {final_blocks} + fc\")\n",
    "            print(f\"   Parameters to train: {final_block_params + fc_params:,}\")\n",
    "            recommendations[name] = {\n",
    "                'strategy': 'Mixed_7* + fc',\n",
    "                'params': final_block_params + fc_params,\n",
    "                'code': 'for name, param in model.named_parameters():\\n    if any(block in name for block in [\"Mixed_7a\", \"Mixed_7b\", \"Mixed_7c\"]) or \"fc\" in name:\\n        param.requires_grad = True'\n",
    "            }\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ðŸ“Š FINE-TUNING PARAMETER COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for name, rec in recommendations.items():\n",
    "        percentage = (rec['params'] / sum(p.numel() for p in models_to_check[name].parameters())) * 100\n",
    "        print(f\"{name:20}: {rec['params']:>10,} params ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Run the analysis\n",
    "recommendations = check_last_stages()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸŽ¯ FINAL RECOMMENDATIONS FOR FAIR COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"To ensure fair comparison, aim for ~5-15% of total parameters to be trainable.\")\n",
    "print(\"Adjust the number of unfrozen layers to achieve similar parameter counts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
